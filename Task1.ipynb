{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4ec9683-f9a8-4296-9d21-9c276d4eafd8",
   "metadata": {},
   "source": [
    "## Download CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15635f77-57a4-49af-95c2-25cbe4608121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "content = requests.get(\"https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv\")\n",
    "with open(\"news_sample.csv\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(content.text)# Read file:\n",
    "news_sample = open(\"news_sample.csv\", \"r\", encoding=\"utf-8\")\n",
    "text = news_sample.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86993bff-3420-4201-98cb-60b358029936",
   "metadata": {},
   "source": [
    "Manually inspect file contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ade039c-8b0d-430f-8405-2ef5ec5742b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c51e38-2a4c-4273-b376-0f5a2b75776c",
   "metadata": {},
   "source": [
    "## Cleaning data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84ec2e06-e439-4719-ac0f-93fc5c2e801f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
     ]
    }
   ],
   "source": [
    "from cleantext import clean\n",
    "cleantext = clean(  text,\n",
    "                    lower=True,\n",
    "                    no_line_breaks=True,\n",
    "                    no_urls=True,\n",
    "                    no_emails=True,\n",
    "                    no_numbers=True,\n",
    "                    replace_with_url=\"URL\",replace_with_email=\"EMAIL\",\n",
    "                    replace_with_number=\"NUM\", #Dates are replaced as numbers.\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c287636-9f9f-4806-9100-22b2417b5870",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(cleantext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3aac29-4775-4920-9f02-2d2de85adbab",
   "metadata": {},
   "source": [
    "## Tokenizing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d63dc20-536d-4ab2-afdb-fef3bed3b754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('all')\n",
    "\n",
    "tokens = nltk.word_tokenize(cleantext)\n",
    "tagged = nltk.pos_tag(tokens) #No idea what this represent\n",
    "#print(tokens)\n",
    "#print(tagged[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074f2c21-8865-4513-8c13-5be63312e576",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b5b129-6a4d-4e2a-98c0-25a20db6ec34",
   "metadata": {},
   "source": [
    "### Creating vocabulary (list without duplicates) and removing stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51f710c3-b8ae-4c02-a4b1-d40f03a83aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "ordered_tokens = set()\n",
    "vocab = []\n",
    "\n",
    "for word in tokens:\n",
    "    if word not in ordered_tokens:\n",
    "        ordered_tokens.add(word)\n",
    "        vocab.append(word)\n",
    "vocab_no_stopwords =[]\n",
    "for word in vocab:\n",
    "    if word not in stopwords.words('english'):\n",
    "        vocab_no_stopwords.append(word)\n",
    "\n",
    "#print(vocab[:100])\n",
    "#print(vocab_no_stopwords[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651a3a02-fd0a-42f8-8c95-2aa16e69a735",
   "metadata": {},
   "source": [
    "Length of vocab before and after removing stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4382eb6a-755f-4d2a-bc81-59a78d77da35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17446\n",
      "17314\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))\n",
    "print(len(vocab_no_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72683419-483f-4304-9933-bf90c0938d2e",
   "metadata": {},
   "source": [
    "Reduction rate of vocabulary size after removing stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "956912f9-a350-46aa-a562-8e0ef614bae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.7566204287515763\n"
     ]
    }
   ],
   "source": [
    "print((len(vocab_no_stopwords)-len(vocab))/len(vocab)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835d244f-d63f-41d8-a971-575e33a74595",
   "metadata": {},
   "source": [
    "### Removing word variations with stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "501b2a82-4687-473a-bb70-17170fe00388",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stemmed_vocab = []\n",
    "\n",
    "for word in vocab_no_stopwords:\n",
    "    if word not in stemmed_vocab: stemmed_vocab.append(stemmer.stem(word))\n",
    "#print(stemmed_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797e31c0-5d13-4881-8dc4-2dbd6dc30195",
   "metadata": {},
   "source": [
    "Length of vocab before and after stemming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4c58824-6e0e-4a67-a102-231049068bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17314\n",
      "16521\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab_no_stopwords))\n",
    "print(len(stemmed_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e75884-9b5c-4088-a020-37f0d51e4c39",
   "metadata": {},
   "source": [
    "Reduction rate of vocabulary size after stemming words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05a7885c-e66a-478f-b435-32e8276dd39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.580108582649879\n"
     ]
    }
   ],
   "source": [
    "print((len(stemmed_vocab)-len(vocab_no_stopwords))/len(vocab_no_stopwords)*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
