{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline\n",
    "import re\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from multiprocessing import Manager\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"995,000_rows.csv\", dtype={0: str, 1: str}) \n",
    "#df = pd.read_csv(\"news_sample.csv\", dtype={0: str, 1: str}) \n",
    "df = df.drop(['keywords', 'tags', 'authors', 'meta_description', 'summary'], axis=1)\n",
    "df = df[df['type'] != '2018-02-10 13:43:39.521661']\n",
    "df = df[df['type'] != 'nan']\n",
    "df = df[df['type'].notna()]\n",
    "df = df[df['content'].str.strip() != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Redacts URLs, dates, email addresses and numbers in a given text input, as well as converting text to lower case and removing tabs, newlines, and spaces following other spaces\"\"\"\n",
    "    text = str(text)\n",
    "    date_exp =  {\n",
    "                \"year_mm_dd\" : re.compile(r'[^\\d]{1}([0-9]{2,4})[\\s\\/\\.\\-\\\\]?([0-1]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-3]{1}[0-9]{1})\\s?([\\d]{2}:[\\d]{2}:[\\d]{2}\\.[\\d]{6})?', re.MULTILINE),   \n",
    "                \"dd_mm_year\" : re.compile(r'[^\\d]{1}([0-3]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-1]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-9]{2,4})\\s?([\\d]{2}:[\\d]{2}:[\\d]{2}\\.[\\d]{6})?', re.MULTILINE),\n",
    "                \"mm_dd_year\" : re.compile(r'[^\\d]{1}([0-1]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-3]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-9]{2,4})\\s?([\\d]{2}:[\\d]{2}:[\\d]{2}\\.[\\d]{6})?', re.MULTILINE),\n",
    "                \"year_mm_dd_time\" : re.compile(r'[^\\d]{1}([0-9]{2,4})[\\s\\/\\.\\-\\\\]?([0-1]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-3]{1}[0-9]{1})\\s?([\\d]{2}:[\\d]{2}:[\\d]{2}\\.[\\d]{6})?', re.MULTILINE),\n",
    "                \"dd_mm_year_time\" : re.compile(r'[^\\d]{1}([0-3]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-1]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-9]{2,4})\\s?([\\d]{2}:[\\d]{2}:[\\d]{2}\\.[\\d]{6})?', re.MULTILINE),\n",
    "                \"mm_dd_year_time\" : re.compile(r'[^\\d]{1}([0-1]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-3]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-9]{2,4})\\s?([\\d]{2}:[\\d]{2}:[\\d]{2}\\.[\\d]{6})?', re.MULTILINE),\n",
    "                \"year_mm_dd_hh_mm\" : re.compile(r'[^\\d]{1}([0-9]{2,4})[\\s\\/\\.\\-\\\\]?([0-1]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-3]{1}[0-9]{1})\\s?([\\d]{2}:[\\d]{2})', re.MULTILINE),\n",
    "                \"dd_mm_year_hh_mm\" : re.compile(r'[^\\d]{1}([0-3]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-1]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-9]{2,4})\\s?([\\d]{2}:[\\d]{2})', re.MULTILINE),\n",
    "                \"mm_dd_year_hh_mm\" : re.compile(r'[^\\d]{1}([0-1]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-3]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-9]{2,4})\\s?([\\d]{2}:[\\d]{2})', re.MULTILINE),\n",
    "                \"year_mm_dd_hh_mm_ss\" : re.compile(r'[^\\d]{1}([0-9]{2,4})[\\s\\/\\.\\-\\\\]?([0-1]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-3]{1}[0-9]{1})\\s?([\\d]{2}:[\\d]{2}:[\\d]{2})', re.MULTILINE),\n",
    "                \"dd_mm_year_hh_mm_ss\" : re.compile(r'[^\\d]{1}([0-3]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-1]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-9]{2,4})\\s?([\\d]{2}:[\\d]{2}:[\\d]{2})', re.MULTILINE),\n",
    "                \"mm_dd_year_hh_mm_ss\" : re.compile(r'[^\\d]{1}([0-1]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-3]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-9]{2,4})\\s?([\\d]{2}:[\\d]{2}:[\\d]{2})', re.MULTILINE),\n",
    "                }\n",
    "    num_exp = re.compile('[0-9]+[,.]?[0-9]*', re.MULTILINE)\n",
    "    num2_exp = re.compile(r'([0-9]+)((st)?(nd)?(rd)?(th)?(st)?){1}')\n",
    "    url_exp = re.compile(r'((h{1}t{2}p{1}s?\\:{1}\\/{2})|(w{3}\\.{1})){0,2}[^,\\s]*\\.[a-zA-Z]{2,}[^,\\s]*', re.MULTILINE)\n",
    "    email_exp = re.compile(r'[^,\\s\\/]*@{1}[^,\\s\\/]*\\.[a-zA-Z]{2,3}', re.MULTILINE)\n",
    "    space_exp = re.compile(r'([\\s]{2,})|[\\t]|[\\n]+', re.MULTILINE)\n",
    "    punctuation_exp = re.compile(r'[^\\w\\s]', re.MULTILINE)\n",
    "\n",
    "    text = text.lower() #Convert to lower case before inserting capitalized placeholders\n",
    "    for exp in date_exp.values():\n",
    "        text = exp.sub('DATETOKEN', text) #Replace dates before numbers\n",
    "    text = num_exp.sub('NUMTOKEN', text)\n",
    "    text = url_exp.sub('URLTOKEN', text)\n",
    "    text = email_exp.sub('EMAILTOKEN', text)\n",
    "    text = space_exp.sub(' ', text)\n",
    "    text = punctuation_exp.sub(' ', text)\n",
    "    return text\n",
    "\n",
    "def clean_text_series(series):\n",
    "    return series.apply(clean_text)\n",
    "\n",
    "def tokenize_and_stem_series(series):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = nltk.PorterStemmer()\n",
    "    return series.apply(lambda x: [stemmer.stem(token) for token in nltk.word_tokenize(x) if token not in stop_words])\n",
    "\n",
    "n = 50000\n",
    "list_df = np.array_split(df, math.ceil(len(df) / n))\n",
    "del df\n",
    "\n",
    "j = 1\n",
    "for df in list_df:\n",
    "    print(f\"Processing chunk {j} of {len(list_df)}:\")\n",
    "    # Check if chunk parquet already exists\n",
    "    try:\n",
    "        pq.read_table(f\"chunk_{j}.parquet\").to_pandas()\n",
    "        print(f\"Chunk {j} already processed!\", flush=True)\n",
    "        df = pd.read_parquet(f\"chunk_{j}.parquet\")\n",
    "        j += 1\n",
    "        continue\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    print(\"Changing column types...\", flush=True)\n",
    "    df['type'] = df['type'].astype('category')\n",
    "    df['domain'] = df['domain'].astype('category')\n",
    "    print(\"Changing datetime columns...\", end=\"\\r\", flush=True)\n",
    "    df['scraped_at'] = pd.to_datetime(df['scraped_at'], format='mixed', utc=True)\n",
    "    df['inserted_at'] = pd.to_datetime(df['inserted_at'], format='mixed')\n",
    "    df['updated_at'] = pd.to_datetime(df['updated_at'], format='mixed')\n",
    "\n",
    "    print(\"Cleaning content...\", flush=True)\n",
    "    df['content'] = clean_text_series(df['content'])\n",
    "    print(\"Tokenizing, stemming and removing stopwords from content...\", flush=True)\n",
    "    df['content'] = tokenize_and_stem_series(df['content'])\n",
    "    print(\"Calculating features...\", end=\"\\r\", flush=True)\n",
    "    df['length'] = df['content'].apply(len)\n",
    "    df['distinct_words'] = df['content'].apply(set)\n",
    "    df['length_distinct_words'] = df['distinct_words'].apply(len)\n",
    "\n",
    "    print(\"Saving chunk...\", flush=True)\n",
    "    df.to_parquet(f\"chunk_{j}.parquet\")\n",
    "    print(f\"Chunk {j} done!\", flush=True)\n",
    "\n",
    "    j += 1\n",
    "\n",
    "df = pd.concat([pd.read_parquet(f\"chunk_{j}.parquet\") for j in range(1, len(list_df) + 1)])\n",
    "del list_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure()\n",
    "ax1 = fig1.add_subplot(111)\n",
    "ax1.bar(df[\"type\"].value_counts().index, df[\"type\"].value_counts().values)\n",
    "ax1.set_title(f'Article types')\n",
    "ax1.set_xlabel('Types')\n",
    "plt.xticks(rotation=45)\n",
    "ax1.set_ylabel('# of articles')\n",
    "fig1.savefig(f'data\\\\articles_of_each_type.png')\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for type in df['type'].unique():\n",
    "    name = f'{type}_df'\n",
    "    globals()[name] = df[df['type'] == type]\n",
    "\n",
    "    print(f'Articles of type {type}: {len(globals()[name])}')\n",
    "\n",
    "    print(f'{type} mean length: {globals()[name][\"length\"].mean()}')\n",
    "    print(f'{type} median length: {globals()[name][\"length\"].median()}')\n",
    "\n",
    "    print(f'{type} mean distinct words: {globals()[name][\"length_distinct_words\"].mean()}')\n",
    "    print(f'{type} median distinct words: {globals()[name][\"length_distinct_words\"].median()}')\n",
    "\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Calculate global min and max for length and length_distinct_words\n",
    "global_min_length = df['length'].min()\n",
    "global_max_length = df['length'].max()\n",
    "global_min_length_distinct_words = df['length_distinct_words'].min()\n",
    "global_max_length_distinct_words = df['length_distinct_words'].max()\n",
    "\n",
    "# Main plot with all data points\n",
    "fig1, ax1 = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Use a color map to automatically assign colors\n",
    "color_map = plt.get_cmap('tab10')\n",
    "\n",
    "# Plot each type with a different color and set the size of the dots\n",
    "dot_size = 1  # Adjust this value to change the size of the dots\n",
    "for i, article_type in enumerate(df['type'].unique()):\n",
    "    type_df = df[df['type'] == article_type]\n",
    "    ax1.scatter(type_df['length_distinct_words'], type_df['length'], color=color_map(i), label=article_type, s=dot_size)\n",
    "\n",
    "ax1.set_title('Article length vs number of distinct words')\n",
    "ax1.set_xlabel('Number of distinct words')\n",
    "ax1.set_ylabel('Article length')\n",
    "ax1.legend()\n",
    "ax1.set_xlim(global_min_length_distinct_words, global_max_length_distinct_words)\n",
    "ax1.set_ylim(global_min_length, global_max_length)\n",
    "\n",
    "# Create a new figure for the grid of subplots\n",
    "num_types = len(df['type'].unique())\n",
    "ncols = 2\n",
    "nrows = (num_types + 1) // ncols  # Calculate the number of rows needed\n",
    "\n",
    "fig2, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(10, 5 * nrows))\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot each type in a separate subplot\n",
    "for i, article_type in enumerate(df['type'].unique()):\n",
    "    type_df = df[df['type'] == article_type]\n",
    "    ax = axes[i]\n",
    "    ax.scatter(type_df['length_distinct_words'], type_df['length'], color=color_map(i), label=article_type, s=dot_size)\n",
    "    ax.set_title(f'{article_type}')\n",
    "    ax.set_xlabel('Number of distinct words')\n",
    "    ax.set_ylabel('Article length')\n",
    "    ax.legend()\n",
    "    ax.set_xlim(global_min_length_distinct_words, global_max_length_distinct_words)\n",
    "    ax.set_ylim(global_min_length, global_max_length)\n",
    "\n",
    "# Hide any unused subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig2.delaxes(axes[j])\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figures\n",
    "fig1.savefig('data/article_length_vs_distinct_words.png')\n",
    "fig2.savefig('data/article_length_vs_distinct_words_by_type.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "output_dir = 'data/article_length_distributions'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Calculate global min and max for length and log(length)\n",
    "global_min_length = df['length'].min()\n",
    "global_max_length = df['length'].max()\n",
    "global_min_log_length = np.log(df['length']).min()\n",
    "global_max_log_length = np.log(df['length']).max()\n",
    "\n",
    "for article_type in df['type'].unique():\n",
    "    fig1 = plt.figure(figsize=(10, 5))\n",
    "    # Adding two subplots side by side\n",
    "    ax1 = fig1.add_subplot(1, 2, 1)\n",
    "    ax2 = fig1.add_subplot(1, 2, 2)\n",
    "    \n",
    "    type_df = df[df['type'] == article_type]\n",
    "    \n",
    "    # The first subplot shows the distribution of the length of the articles\n",
    "    ax1.hist(type_df['length'], bins=100)\n",
    "    ax1.set_title(f'{article_type} article length distribution')\n",
    "    ax1.set_xlabel('Article length')\n",
    "    ax1.set_ylabel('# of articles')\n",
    "    ax1.set_xlim(global_min_length, global_max_length)  # Set uniform x-axis limits\n",
    "\n",
    "    # The second subplot shows the distribution of the log of the length of the articles to better visualize the distribution\n",
    "    ax2.hist(np.log(type_df['length']), bins=100)\n",
    "    ax2.set_title(f'{article_type} article length distribution (log scale)')\n",
    "    ax2.set_xlabel('log(Article length)')\n",
    "    ax2.set_ylabel('# of articles')\n",
    "    ax2.set_xlim(global_min_log_length, global_max_log_length)  # Set uniform x-axis limits\n",
    "\n",
    "    fig1.savefig(f'{output_dir}/{article_type}_article_length_distribution.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = {}\n",
    "for content in df['content']:\n",
    "    for word in content:\n",
    "        if word in word_counts:\n",
    "            word_counts[word] += 1\n",
    "        else:\n",
    "            word_counts[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(20, 5))\n",
    "ax1 = fig1.add_subplot(111)\n",
    "ax1.bar(*zip(*sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:100]))\n",
    "ax1.set_title('10 most common words')\n",
    "ax1.set_xlabel('Words')\n",
    "plt.xticks(rotation=45)\n",
    "ax1.set_ylabel('# of occurrences')\n",
    "fig1.savefig('data/10_most_common_words.png')\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
