{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline\n",
    "import re\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '995,000_rows.csv'\n",
    "df = pd.read_csv(filename, usecols=['content', 'type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Redacts URLs, dates, email addresses and numbers in a given text input, as well as converting text to lower case and removing tabs, newlines, and spaces following other spaces\"\"\"\n",
    "    text = str(text)\n",
    "    date_exp =  {\n",
    "                \"year_mm_dd\" : re.compile(r'[^\\d]{1}([0-9]{2,4})[\\s\\/\\.\\-\\\\]?([0-1]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-3]{1}[0-9]{1})\\s?([\\d]{2}:[\\d]{2}:[\\d]{2}\\.[\\d]{6})?', re.MULTILINE),   \n",
    "                \"dd_mm_year\" : re.compile(r'[^\\d]{1}([0-3]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-1]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-9]{2,4})\\s?([\\d]{2}:[\\d]{2}:[\\d]{2}\\.[\\d]{6})?', re.MULTILINE),\n",
    "                \"mm_dd_year\" : re.compile(r'[^\\d]{1}([0-1]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-3]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-9]{2,4})\\s?([\\d]{2}:[\\d]{2}:[\\d]{2}\\.[\\d]{6})?', re.MULTILINE),\n",
    "                \"year_mm_dd_time\" : re.compile(r'[^\\d]{1}([0-9]{2,4})[\\s\\/\\.\\-\\\\]?([0-1]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-3]{1}[0-9]{1})\\s?([\\d]{2}:[\\d]{2}:[\\d]{2}\\.[\\d]{6})?', re.MULTILINE),\n",
    "                \"dd_mm_year_time\" : re.compile(r'[^\\d]{1}([0-3]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-1]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-9]{2,4})\\s?([\\d]{2}:[\\d]{2}:[\\d]{2}\\.[\\d]{6})?', re.MULTILINE),\n",
    "                \"mm_dd_year_time\" : re.compile(r'[^\\d]{1}([0-1]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-3]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-9]{2,4})\\s?([\\d]{2}:[\\d]{2}:[\\d]{2}\\.[\\d]{6})?', re.MULTILINE),\n",
    "                \"year_mm_dd_hh_mm\" : re.compile(r'[^\\d]{1}([0-9]{2,4})[\\s\\/\\.\\-\\\\]?([0-1]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-3]{1}[0-9]{1})\\s?([\\d]{2}:[\\d]{2})', re.MULTILINE),\n",
    "                \"dd_mm_year_hh_mm\" : re.compile(r'[^\\d]{1}([0-3]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-1]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-9]{2,4})\\s?([\\d]{2}:[\\d]{2})', re.MULTILINE),\n",
    "                \"mm_dd_year_hh_mm\" : re.compile(r'[^\\d]{1}([0-1]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-3]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-9]{2,4})\\s?([\\d]{2}:[\\d]{2})', re.MULTILINE),\n",
    "                \"year_mm_dd_hh_mm_ss\" : re.compile(r'[^\\d]{1}([0-9]{2,4})[\\s\\/\\.\\-\\\\]?([0-1]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-3]{1}[0-9]{1})\\s?([\\d]{2}:[\\d]{2}:[\\d]{2})', re.MULTILINE),\n",
    "                \"dd_mm_year_hh_mm_ss\" : re.compile(r'[^\\d]{1}([0-3]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-1]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-9]{2,4})\\s?([\\d]{2}:[\\d]{2}:[\\d]{2})', re.MULTILINE),\n",
    "                \"mm_dd_year_hh_mm_ss\" : re.compile(r'[^\\d]{1}([0-1]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-3]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-9]{2,4})\\s?([\\d]{2}:[\\d]{2}:[\\d]{2})', re.MULTILINE),\n",
    "                }\n",
    "    num2_exp = re.compile(r'([0-9]+)((st)?(nd)?(rd)?(th)?(st)?){1}')\n",
    "    num_exp = re.compile('[0-9]+[,.]?[0-9]*', re.MULTILINE)\n",
    "    url_exp = re.compile(r'((h{1}t{2}p{1}s?\\:{1}\\/{2})|(w{3}\\.{1})){0,2}[^,\\s]*\\.[a-zA-Z]{2,}[^,\\s]*', re.MULTILINE)\n",
    "    email_exp = re.compile(r'[^,\\s\\/]*@{1}[^,\\s\\/]*\\.[a-zA-Z]{2,3}', re.MULTILINE)\n",
    "    space_exp = re.compile(r'([\\s]{2,})|[\\t]|[\\n]+', re.MULTILINE)\n",
    "    punctuation_exp = re.compile(r'[^\\w\\s]', re.MULTILINE)\n",
    "\n",
    "    text = text.lower()\n",
    "    for exp in date_exp.values():\n",
    "        text = exp.sub('datetoken', text) #Replace dates before numbers\n",
    "    text = num2_exp.sub('numtoken', text)\n",
    "    text = num_exp.sub('numtoken', text)\n",
    "    text = url_exp.sub('urltoken', text)\n",
    "    text = email_exp.sub('emailtoken', text)\n",
    "    text = space_exp.sub(' ', text)\n",
    "    text = punctuation_exp.sub(' ', text)\n",
    "    return text\n",
    "\n",
    "def clean_text_series(series):\n",
    "    return series.apply(clean_text)\n",
    "\n",
    "def tokenize_and_stem_series(series):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = nltk.PorterStemmer()\n",
    "    return series.apply(lambda x: [stemmer.stem(token) for token in nltk.word_tokenize(x) if token not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_NUM(x):\n",
    "    count = 0\n",
    "    for word in x:\n",
    "        if word == 'numtoken':\n",
    "            count += 1\n",
    "    return count\n",
    "def f_URL(x):\n",
    "    count = 0\n",
    "    for word in x:\n",
    "        if word == 'urltoken':\n",
    "            count += 1\n",
    "    return count\n",
    "    \n",
    "def f_EMAIL(x):\n",
    "    count = 0\n",
    "    for word in x:\n",
    "        if word == 'emailtoken':\n",
    "            count += 1\n",
    "    return count\n",
    "def f_DATE(x):\n",
    "    count = 0\n",
    "    for word in x:\n",
    "        if word == 'datetoken':\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def df_chunker(df, chunksize):\n",
    "    list_df = np.array_split(df, math.ceil(len(df) / chunksize))\n",
    "    del df\n",
    "    j = 1\n",
    "    for df in list_df:\n",
    "        print(f\"Processing chunk {j} of {len(list_df)}:\")\n",
    "        # Check if chunk parquet already exists\n",
    "        try:\n",
    "            pq.read_table(f\"chunk_{j}.parquet\").to_pandas()\n",
    "            print(f\"Chunk {j} already processed!\", flush=True)\n",
    "            chunk = pd.read_parquet(f\"chunk_{j}.parquet\")\n",
    "            j += 1\n",
    "            continue\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        print(\"Cleaning content...\", flush=True)\n",
    "        df['content'] = clean_text_series(df['content'])\n",
    "        print(\"\\n\", end=\"\\r\", flush=True)\n",
    "\n",
    "        print(\"Tokenizing, stemming and removing stopwords from content...\", flush=True)\n",
    "        df['content'] = tokenize_and_stem_series(df['content'])\n",
    "        print(\"\\n\", end=\"\\r\", flush=True)\n",
    "\n",
    "        print(\"Calculating features...\", end=\"\\r\", flush=True)\n",
    "        df['length'] = df['content'].apply(len)\n",
    "        df['distinct_words'] = df['content'].apply(set)\n",
    "        df['length_distinct_words'] = df['distinct_words'].apply(len)\n",
    "        df['group'] = df['type'].apply(lambda x: 1 if x in ['fake', 'satire', 'bias', 'conspiracy', 'junksci', 'hate'] else 0)\n",
    "\n",
    "        df['numtokens'] = df['content'].apply(f_NUM)\n",
    "        df['urltokens'] = df['content'].apply(f_URL)\n",
    "        df['emailtokens'] = df['content'].apply(f_EMAIL)\n",
    "        df['datetokens'] = df['content'].apply(f_DATE)\n",
    "        print(\"\\n\", end=\"\\r\", flush=True)\n",
    "\n",
    "        print(\"Saving chunk...\", end=\"\\r\", flush=True)\n",
    "        df.to_parquet(f\"chunk_{j}.parquet\")\n",
    "        print(f\"Chunk {j} done!\", flush=True)\n",
    "        \n",
    "        j += 1\n",
    "    df = pd.concat([pd.read_parquet(f\"chunk_{j}.parquet\") for j in range(1, (len(list_df) + 1))])\n",
    "    del list_df\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fesso\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\numpy\\_core\\fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1 of 20:\n",
      "Chunk 1 already processed!\n",
      "Processing chunk 2 of 20:\n",
      "Chunk 2 already processed!\n",
      "Processing chunk 3 of 20:\n",
      "Chunk 3 already processed!\n",
      "Processing chunk 4 of 20:\n",
      "Chunk 4 already processed!\n",
      "Processing chunk 5 of 20:\n",
      "Chunk 5 already processed!\n",
      "Processing chunk 6 of 20:\n",
      "Chunk 6 already processed!\n",
      "Processing chunk 7 of 20:\n",
      "Chunk 7 already processed!\n",
      "Processing chunk 8 of 20:\n",
      "Chunk 8 already processed!\n",
      "Processing chunk 9 of 20:\n",
      "Chunk 9 already processed!\n",
      "Processing chunk 10 of 20:\n",
      "Chunk 10 already processed!\n",
      "Processing chunk 11 of 20:\n",
      "Chunk 11 already processed!\n",
      "Processing chunk 12 of 20:\n",
      "Chunk 12 already processed!\n",
      "Processing chunk 13 of 20:\n",
      "Chunk 13 already processed!\n",
      "Processing chunk 14 of 20:\n",
      "Chunk 14 already processed!\n",
      "Processing chunk 15 of 20:\n",
      "Chunk 15 already processed!\n",
      "Processing chunk 16 of 20:\n",
      "Chunk 16 already processed!\n",
      "Processing chunk 17 of 20:\n",
      "Chunk 17 already processed!\n",
      "Processing chunk 18 of 20:\n",
      "Chunk 18 already processed!\n",
      "Processing chunk 19 of 20:\n",
      "Chunk 19 already processed!\n",
      "Processing chunk 20 of 20:\n",
      "Cleaning content...\n",
      "\n",
      "Tokenizing, stemming and removing stopwords from content...\n",
      "\n",
      "Calculating features...\n",
      "Chunk 20 done!.\n"
     ]
    }
   ],
   "source": [
    "df = df_chunker(df, 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
