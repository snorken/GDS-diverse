{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from cleantext import clean\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '995,000_rows.csv'\n",
    "#filename = 'news_sample.csv'\n",
    "chunk_size = 50000\n",
    "df = pd.read_csv(filename, chunksize=chunk_size, usecols=['content', 'type', 'title', 'domain', 'authors', 'meta_keywords'])\n",
    "output_dir = 'data/chunks'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_exp =  {   \"year_mm_dd\" : re.compile(r'[^\\d]{1}([0-9]{2,4})[\\s\\/\\.\\-\\\\]?([0-1]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-3]{1}[0-9]{1})\\s?([\\d]{2}:[\\d]{2}:[\\d]{2}\\.[\\d]{6})?', re.MULTILINE),   \n",
    "                \"dd_mm_year\" : re.compile(r'[^\\d]{1}([0-3]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-1]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-9]{2,4})\\s?([\\d]{2}:[\\d]{2}:[\\d]{2}\\.[\\d]{6})?', re.MULTILINE),\n",
    "                \"mm_dd_year\" : re.compile(r'[^\\d]{1}([0-1]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-3]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-9]{2,4})\\s?([\\d]{2}:[\\d]{2}:[\\d]{2}\\.[\\d]{6})?', re.MULTILINE),\n",
    "                \"year_mm_dd_time\" : re.compile(r'[^\\d]{1}([0-9]{2,4})[\\s\\/\\.\\-\\\\]?([0-1]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-3]{1}[0-9]{1})\\s?([\\d]{2}:[\\d]{2}:[\\d]{2}\\.[\\d]{6})?', re.MULTILINE),\n",
    "                \"dd_mm_year_time\" : re.compile(r'[^\\d]{1}([0-3]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-1]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-9]{2,4})\\s?([\\d]{2}:[\\d]{2}:[\\d]{2}\\.[\\d]{6})?', re.MULTILINE),\n",
    "                \"mm_dd_year_time\" : re.compile(r'[^\\d]{1}([0-1]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-3]{1}[0-9]{1})[\\s\\/\\.\\-\\\\]?([0-9]{2,4})\\s?([\\d]{2}:[\\d]{2}:[\\d]{2}\\.[\\d]{6})?', re.MULTILINE),\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 20...\r"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = nltk.PorterStemmer()\n",
    "\n",
    "def token_count(x, token):\n",
    "    count = 0\n",
    "    for word in x:\n",
    "        if word == token:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "i = 1\n",
    "\n",
    "for chunk in df: \n",
    "    print(f\"Processing chunk {i}...\", end = '\\r', flush=True)\n",
    "    #Check if chunk parquet already exists\n",
    "    try:\n",
    "        pq.read_table(f\"{output_dir}/chunk_{i}.parquet\").to_pandas()\n",
    "        print(f\"Chunk {i} already processed!\", flush=True)\n",
    "        i += 1\n",
    "        continue\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    #Drop empty rows\n",
    "    chunk = chunk.dropna(subset=['content', 'type'])\n",
    "\n",
    "    #Drop that one weird row\n",
    "    chunk = chunk[chunk['type'] != '2018-02-10 13:43:39.521661']\n",
    "\n",
    "    #Processing content column. Cleaning, tokenizing and stemming.\n",
    "    def date_replace(x):\n",
    "        for exp in date_exp.values():\n",
    "            x = exp.sub('datetoken', x)\n",
    "        return x\n",
    "\n",
    "    chunk['content'] = chunk['content'].transform(date_replace)\n",
    "\n",
    "    chunk['content'] = chunk['content'].transform(lambda x: clean(x,\n",
    "                                        lower=True,\n",
    "                                        no_line_breaks=True,\n",
    "                                        no_urls=True,\n",
    "                                        no_emails=True,\n",
    "                                        no_numbers=True,\n",
    "                                        no_punct=True,\n",
    "                                        no_currency_symbols=True,\n",
    "                                        normalize_whitespace=True,\n",
    "                                        replace_with_currency_symbol=\"currencytoken\",\n",
    "                                        replace_with_url=\"urltoken\",\n",
    "                                        replace_with_email=\"emailtoken\",\n",
    "                                        replace_with_number=\"numtoken\",\n",
    "                                        replace_with_punct=\"\",\n",
    "                                        lang=\"en\")\n",
    "                                        )\n",
    "    chunk['title'] = chunk['title'].transform(lambda x: clean(x,\n",
    "                                        lower=True,\n",
    "                                        no_line_breaks=True,\n",
    "                                        no_urls=True,\n",
    "                                        no_emails=True,\n",
    "                                        no_numbers=True,\n",
    "                                        no_punct=True,\n",
    "                                        no_currency_symbols=True,\n",
    "                                        normalize_whitespace=True,\n",
    "                                        replace_with_currency_symbol=\"currencytoken\",\n",
    "                                        replace_with_url=\"urltoken\",\n",
    "                                        replace_with_email=\"emailtoken\",\n",
    "                                        replace_with_number=\"numtoken\",\n",
    "                                        replace_with_punct=\"\",\n",
    "                                        lang=\"en\")\n",
    "                                        )\n",
    "    chunk['content'] = chunk['content'].transform(lambda x: [stemmer.stem(token) for token in word_tokenize(x) if token not in stop_words])\n",
    "    chunk['title'] = chunk['title'].transform(lambda x: [stemmer.stem(token) for token in word_tokenize(x) if token not in stop_words])\n",
    "\n",
    "    #Calculating number of numtokens, urltokens, emailtokens and datetokens\n",
    "    chunk['num_count'] = chunk['content'].transform(lambda x: token_count(x, 'numtoken'))\n",
    "    chunk['url_count'] = chunk['content'].transform(lambda x: token_count(x, 'urltoken'))\n",
    "    chunk['email_count'] = chunk['content'].transform(lambda x: token_count(x, 'emailtoken'))\n",
    "    chunk['date_count'] = chunk['content'].transform(lambda x: token_count(x, 'datetoken'))\n",
    "    \n",
    "    #Calculating article length, distinct words, and number of distinct words.\n",
    "    chunk['length'] = chunk['content'].apply(len)\n",
    "    chunk['distinct_words'] = chunk['content'].apply(set)\n",
    "    chunk['length_distinct_words'] = chunk['distinct_words'].apply(len)\n",
    "\n",
    "    #Dropping rows with length 0 (cleaning)\n",
    "    chunk = chunk[chunk['length'] != 0]\n",
    "\n",
    "    #Assigning group based on article type\n",
    "    chunk['group'] = chunk['type'].apply(lambda x: 1 if x in ['fake', 'satire', 'bias', 'conspiracy', 'junksci', 'hate'] else 0)\n",
    "\n",
    "    chunk.to_parquet(f\"{output_dir}/chunk_{i}.parquet\")\n",
    "    i += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
