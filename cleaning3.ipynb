{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from cleantext import clean\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '995,000_rows.csv'\n",
    "#filename = 'news_sample.csv'\n",
    "chunk_size = 50000\n",
    "df = pd.read_csv(filename, chunksize=chunk_size, usecols=['content', 'type', 'title', 'domain', 'authors', 'meta_keywords', 'authors'])\n",
    "output_dir = 'data/chunks'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 20...\r"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = nltk.PorterStemmer()\n",
    "\n",
    "def clean_data(x):\n",
    "    x = str(x)\n",
    "    return clean(x,\n",
    "                 lower=True,\n",
    "                 no_line_breaks=True,\n",
    "                 no_urls=True,\n",
    "                 no_emails=True,\n",
    "                 no_numbers=True,\n",
    "                 no_punct=True,\n",
    "                 no_currency_symbols=True,\n",
    "                 normalize_whitespace=True,\n",
    "                 replace_with_currency_symbol=\"currencytoken\",\n",
    "                 replace_with_url=\"urltoken\",\n",
    "                 replace_with_email=\"emailtoken\",\n",
    "                 replace_with_number=\"numtoken\",\n",
    "                 replace_with_punct=\"\",\n",
    "                 lang=\"en\")\n",
    "\n",
    "def process_data(x):\n",
    "    return [stemmer.stem(token) for token in word_tokenize(x) if token not in stop_words]\n",
    "\n",
    "i = 1\n",
    "\n",
    "for chunk in df: \n",
    "    print(f\"Processing chunk {i}...\", end = '\\r', flush=True)\n",
    "    #Check if chunk parquet already exists\n",
    "    try:\n",
    "        pq.read_table(f\"{output_dir}/chunk_{i}.parquet\").to_pandas()\n",
    "        print(f\"Chunk {i} already processed!\", flush=True)\n",
    "        i += 1\n",
    "        continue\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    #Drop rows with no content or type\n",
    "    chunk = chunk.dropna(subset=['content', 'type'])\n",
    "\n",
    "    #Drop that one weird row\n",
    "    chunk = chunk[chunk['type'] != '2018-02-10 13:43:39.521661']\n",
    "\n",
    "    #Cleaning data\n",
    "    chunk['content'] = chunk['content'].transform(clean_data)\n",
    "    chunk['title'] = chunk['title'].transform(clean_data)\n",
    "    chunk['authors'] = chunk['authors'].transform(clean_data)\n",
    "    chunk['meta_keywords'] = chunk['meta_keywords'].transform(clean_data)\n",
    "\n",
    "    #Tokenizing and stemming data\n",
    "    chunk['content'] = chunk['content'].transform(process_data)\n",
    "    chunk['title'] = chunk['title'].transform(process_data)\n",
    "    chunk['meta_keywords'] = chunk['meta_keywords'].transform(process_data)\n",
    "\n",
    "    chunk['authors'] = chunk['authors'].transform(word_tokenize) #Not stemming authors\n",
    "    \n",
    "    #Calculating article length, distinct words, and number of distinct words.\n",
    "    chunk['length'] = chunk['content'].apply(len)\n",
    "    chunk['distinct_words'] = chunk['content'].apply(set)\n",
    "    chunk['length_distinct_words'] = chunk['distinct_words'].apply(len)\n",
    "\n",
    "    #Dropping rows with length 0 (after cleaning)\n",
    "    chunk = chunk[chunk['length'] != 0]\n",
    "\n",
    "    #Assigning group based on article type\n",
    "    chunk['group'] = chunk['type'].apply(lambda x: 1 if x in ['fake', 'satire', 'bias', 'conspiracy', 'junksci', 'hate'] else 0)\n",
    "\n",
    "    chunk.to_parquet(f\"{output_dir}/chunk_{i}.parquet\")\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_df = pd.read_csv('bbc_news.csv', usecols=['article_text', 'title', 'authors'])\n",
    "bbc_df['content'] = bbc_df['article_text']\n",
    "bbc_df['domain'] = 'bbc.com'\n",
    "bbc_df['meta_keywords'] = ''\n",
    "bbc_df['type'] = 'reliable'\n",
    "bbc_df['group'] = 0\n",
    "bbc_df.drop(columns=['article_text'], inplace=True)\n",
    "\n",
    "bbc_df['content'] = bbc_df['content'].transform(clean_data)\n",
    "bbc_df['title'] = bbc_df['title'].transform(clean_data)\n",
    "bbc_df['authors'] = bbc_df['authors'].transform(clean_data)\n",
    "\n",
    "bbc_df['content'] = bbc_df['content'].transform(process_data)\n",
    "bbc_df['title'] = bbc_df['title'].transform(process_data)\n",
    "bbc_df['meta_keywords'] = bbc_df['meta_keywords'].transform(process_data)\n",
    "bbc_df['authors'] = bbc_df['authors'].transform(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
