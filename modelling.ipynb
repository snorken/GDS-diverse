{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([pd.read_parquet(f\"data/chunks/chunk_{j}.parquet\") for j in (range(1, 20))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning lists back into strings\n",
    "df['content'] = df['content'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the vocabulary matrix\n",
    "vectorizer = CountVectorizer(max_features=10000)\n",
    "x = vectorizer.fit_transform(df['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting 80/10/10\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, df['group'], test_size=0.2, random_state=1337)\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5, random_state=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.90      0.87     54736\n",
      "           1       0.82      0.74      0.78     35487\n",
      "\n",
      "    accuracy                           0.83     90223\n",
      "   macro avg       0.83      0.82      0.82     90223\n",
      "weighted avg       0.83      0.83      0.83     90223\n",
      "\n",
      "True negatives: 49098\n",
      "False positives: 5638\n",
      "False negatives: 9314\n",
      "True positives: 26173\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression(max_iter=1000)\n",
    "classifier.fit(x_train, y_train)\n",
    "y_pred = classifier.predict(x_val)\n",
    "print(classification_report(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tn, fp, fn, tp) = confusion_matrix(y_val, y_pred).ravel()\n",
    "print(f'True negatives: {tn}')\n",
    "print(f'False positives: {fp}')\n",
    "print(f'False negatives: {fn}')\n",
    "print(f'True positives: {tp}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nedenfor har jeg prøvet med flere features. Som jeg nævnte er det næsten samme resultat, og det er gået op for mig hvorfor. \n",
    "De ekstra features jeg har prøvet, altså antallet af de forskellige tokens, er i forvejen indeholdt i matrixen, så der er ingen egentlig ekstra information at træne på. D'oh! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.90      0.87     54736\n",
      "           1       0.82      0.74      0.78     35487\n",
      "\n",
      "    accuracy                           0.83     90223\n",
      "   macro avg       0.83      0.82      0.82     90223\n",
      "weighted avg       0.83      0.83      0.83     90223\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fesso\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "text_features = vectorizer.fit_transform(df['content'])\n",
    "numerical_features = df[['num_count', 'url_count', 'email_count', 'date_count']]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_numerical_features = scaler.fit_transform(numerical_features)\n",
    "\n",
    "x_features = hstack([text_features, scaled_numerical_features])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_features, df['group'], test_size=0.2, random_state=1337)\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5, random_state=1337)\n",
    "\n",
    "classifier.fit(x_train, y_train)\n",
    "y_pred = classifier.predict(x_val)\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Including the length of the articles and the number of distinct words in the article improves the f1 score by 1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.89      0.87     54736\n",
      "           1       0.82      0.76      0.79     35487\n",
      "\n",
      "    accuracy                           0.84     90223\n",
      "   macro avg       0.83      0.82      0.83     90223\n",
      "weighted avg       0.84      0.84      0.84     90223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_features = vectorizer.fit_transform(df['content'])\n",
    "numerical_features = df[['length', 'length_distinct_words']]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_numerical_features = scaler.fit_transform(numerical_features)\n",
    "\n",
    "x_features = hstack([text_features, scaled_numerical_features])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_features, df['group'], test_size=0.2, random_state=1337)\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5, random_state=1337)\n",
    "\n",
    "classifier.fit(x_train, y_train)\n",
    "y_pred = classifier.predict(x_val)\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
