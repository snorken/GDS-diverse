{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from scipy.sparse import hstack\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([pd.read_parquet(f\"data/chunks/chunk_{j}.parquet\") for j in (range(1, 21))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_df = pd.read_csv('995,000_rows.csv', usecols=['domain'])\n",
    "def f1(x):\n",
    "    x = str(x)\n",
    "    a = x.split('.')\n",
    "    return a[0]\n",
    "df['domain'] = domain_df['domain'].apply(lambda x: f1(x))\n",
    "del domain_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning lists back into strings\n",
    "df['content'] = df['content'].apply(lambda x: ' '.join(x))\n",
    "df['title'] = df['title'].apply(lambda x: ' '.join(x))\n",
    "df['meta_keywords'] = df['meta_keywords'].apply(lambda x: ' '.join(x))\n",
    "df['authors'] = df['authors'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just the content column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the vocabulary matrix\n",
    "vectorizer = CountVectorizer(max_features=10000)\n",
    "x = vectorizer.fit_transform(df['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting 80/10/10\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, df['group'], test_size=0.2, random_state=1337)\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5, random_state=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.91      0.87     57439\n",
      "           1       0.83      0.74      0.78     37283\n",
      "\n",
      "    accuracy                           0.84     94722\n",
      "   macro avg       0.84      0.82      0.83     94722\n",
      "weighted avg       0.84      0.84      0.84     94722\n",
      "\n",
      "True negatives: 51984\n",
      "False positives: 5455\n",
      "False negatives: 9797\n",
      "True positives: 27486\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression(max_iter=5000)\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(x_val)\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "(tn, fp, fn, tp) = confusion_matrix(y_val, y_pred).ravel()\n",
    "print(f'True negatives: {tn}')\n",
    "print(f'False positives: {fp}')\n",
    "print(f'False negatives: {fn}')\n",
    "print(f'True positives: {tp}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With additional features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = vectorizer.fit_transform(df['content'])\n",
    "title = vectorizer.fit_transform(df['title'])\n",
    "meta_keywords = vectorizer.fit_transform(df['meta_keywords'])\n",
    "authors = vectorizer.fit_transform(df['authors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96     57439\n",
      "           1       0.94      0.94      0.94     37283\n",
      "\n",
      "    accuracy                           0.95     94722\n",
      "   macro avg       0.95      0.95      0.95     94722\n",
      "weighted avg       0.95      0.95      0.95     94722\n",
      "\n",
      "True negatives: 55033\n",
      "False positives: 2406\n",
      "False negatives: 2267\n",
      "True positives: 35016\n"
     ]
    }
   ],
   "source": [
    "x_features = hstack([title, content, meta_keywords, authors])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_features, df['group'], test_size=0.2, random_state=1337)\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5, random_state=1337)\n",
    "\n",
    "classifier.fit(x_train, y_train)\n",
    "y_pred = classifier.predict(x_val)\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "(tn, fp, fn, tp) = confusion_matrix(y_val, y_pred).ravel()\n",
    "print(f'True negatives: {tn}')\n",
    "print(f'False positives: {fp}')\n",
    "print(f'False negatives: {fn}')\n",
    "print(f'True positives: {tp}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With domains (cheating!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     57439\n",
      "           1       1.00      1.00      1.00     37283\n",
      "\n",
      "    accuracy                           1.00     94722\n",
      "   macro avg       1.00      1.00      1.00     94722\n",
      "weighted avg       1.00      1.00      1.00     94722\n",
      "\n",
      "True negatives: 57417\n",
      "False positives: 22\n",
      "False negatives: 23\n",
      "True positives: 37260\n"
     ]
    }
   ],
   "source": [
    "domain = vectorizer.fit_transform(df['domain'])\n",
    "\n",
    "x_features = hstack([title, content, meta_keywords, authors, domain])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_features, df['group'], test_size=0.2, random_state=1337)\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5, random_state=1337)\n",
    "\n",
    "classifier.fit(x_train, y_train)\n",
    "y_pred = classifier.predict(x_val)\n",
    "\n",
    "print(classification_report(y_val, y_pred))\n",
    "(tn, fp, fn, tp) = confusion_matrix(y_val, y_pred).ravel()\n",
    "print(f'True negatives: {tn}')\n",
    "print(f'False positives: {fp}')\n",
    "print(f'False negatives: {fn}')\n",
    "print(f'True positives: {tp}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reliable_count, unreliable_count = y_train.value_counts()\n",
    "total_count = reliable_count + unreliable_count\n",
    "reliable_weight = total_count / reliable_count\n",
    "unreliable_weight = total_count / unreliable_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training set is roughly 60% reliable articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.76      0.81     57439\n",
      "           1       0.69      0.81      0.74     37283\n",
      "\n",
      "    accuracy                           0.78     94722\n",
      "   macro avg       0.78      0.79      0.78     94722\n",
      "weighted avg       0.79      0.78      0.78     94722\n",
      "\n",
      "True negatives: 43920\n",
      "False positives: 13519\n",
      "False negatives: 7156\n",
      "True positives: 30127\n"
     ]
    }
   ],
   "source": [
    "x_features = hstack([content])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_features, df['group'], test_size=0.2, random_state=1337)\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5, random_state=1337)\n",
    "\n",
    "classifier = MultinomialNB(class_prior=[reliable_weight, unreliable_weight], alpha=0.01)\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = classifier.predict(x_val)\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "(tn, fp, fn, tp) = confusion_matrix(y_val, y_pred).ravel()\n",
    "print(f'True negatives: {tn}')\n",
    "print(f'False positives: {fp}')\n",
    "print(f'False negatives: {fn}')\n",
    "print(f'True positives: {tp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.86      0.90     57439\n",
      "           1       0.81      0.90      0.85     37283\n",
      "\n",
      "    accuracy                           0.88     94722\n",
      "   macro avg       0.87      0.88      0.87     94722\n",
      "weighted avg       0.88      0.88      0.88     94722\n",
      "\n",
      "True negatives: 49592\n",
      "False positives: 7847\n",
      "False negatives: 3660\n",
      "True positives: 33623\n"
     ]
    }
   ],
   "source": [
    "x_features = hstack([title, content, meta_keywords, authors])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_features, df['group'], test_size=0.2, random_state=1337)\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5, random_state=1337)\n",
    "\n",
    "classifier = MultinomialNB(class_prior=[reliable_weight, unreliable_weight], alpha=0.01)\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = classifier.predict(x_val)\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "(tn, fp, fn, tp) = confusion_matrix(y_val, y_pred).ravel()\n",
    "print(f'True negatives: {tn}')\n",
    "print(f'False positives: {fp}')\n",
    "print(f'False negatives: {fn}')\n",
    "print(f'True positives: {tp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
